# Autonomous Brainstorming & Ideation

## Purpose

This document equips Atlas UX agents with the cognitive machinery to generate high-quality ideas without waiting for human prompting. Autonomous ideation is not random — it is disciplined creativity operating within strategic constraints. Every agent in the roster should be capable of self-initiated ideation within their domain, contributing to a collective intelligence that exceeds any individual contributor.

## When Agents Should Self-Initiate Brainstorming

Agents should autonomously trigger a brainstorming cycle when any of these conditions are detected:

- A KPI deviates more than 15% from its 7-day moving average (positive or negative)
- A competitor signal surfaces in Daily-Intel or Archy's research feed
- A customer support pattern repeats 3+ times in a 48-hour window (Cheryl escalates)
- A content piece outperforms its cohort by 2x+ engagement (Sunday investigates why)
- A workflow fails at the same step across multiple executions (Petra flags)
- Revenue or pipeline velocity changes slope (Binky detects)
- A regulatory or compliance signal appears (Jenny or Tina flags)

The trigger is not "someone told me to think." The trigger is "the data changed and I need to respond."

---

## Divergent Thinking: SCAMPER Framework

SCAMPER is the primary divergent thinking tool. Each letter represents a cognitive operation applied to an existing product, process, or strategy.

### S — Substitute

What component, material, person, process, or resource can be replaced? What alternative inputs produce different outputs?

Agent application: Binky evaluates whether substituting one acquisition channel for another yields better CAC. Sunday asks whether substituting a blog format for a video format reaches an underserved segment. Tina asks whether substituting a payment provider reduces transaction fees.

### C — Combine

What ideas, features, processes, or audiences can be merged? What happens when you blend two things that haven't been blended before?

Agent application: Mercer combines outreach templates with personalized research data. Petra combines two sequential workflows into a single parallel one. Daily-Intel combines signals from multiple platforms into a single narrative.

### A — Adapt

What else is like this? What ideas from other domains, industries, or time periods can be borrowed and modified?

Agent application: Archy researches how a strategy that worked in fintech might apply to Atlas UX's SaaS context. Cheryl adapts hospitality-industry service recovery techniques to software support.

### M — Modify (Magnify / Minimize)

What can be made larger, smaller, faster, slower, more frequent, less frequent? What attribute can be exaggerated or reduced?

Agent application: Sunday tests whether doubling post frequency increases or decreases engagement per post. Binky models what happens if the free trial is shortened from 14 days to 7. Atlas asks what happens if the agent roster is halved — which agents are load-bearing?

### P — Put to Other Uses

Can this be used in a different context, for a different audience, or to solve a different problem?

Agent application: Content created for blog SEO gets repurposed as email sequences (Reynolds to Sunday pipeline). Customer support transcripts become training data for better agent responses. Audit trail data becomes compliance documentation.

### E — Eliminate

What can be removed without reducing value? What steps, features, or processes exist only because of inertia?

Agent application: Petra audits workflows for steps that add latency but not value. Tina identifies expenses that no longer contribute to outcomes. Atlas challenges any process that exists because "we've always done it that way."

### R — Reverse (Rearrange)

What happens if you reverse the order, flip the relationship, or swap roles? What if the opposite were true?

Agent application: Instead of agents pushing content to platforms, what if platforms pull from a central content API? Instead of Cheryl reacting to support tickets, what if she proactively reaches out before problems occur? Instead of Atlas assigning tasks, what if agents bid on tasks based on capacity?

---

## Convergent Thinking: From Many Ideas to the Best Ideas

Divergent thinking generates volume. Convergent thinking generates value. These methods narrow the field.

### Affinity Mapping

Group related ideas into clusters. Label each cluster with a theme. Identify which cluster contains the highest density of promising ideas. In multi-agent context: each agent tags their ideas with domain labels. Sunday groups cross-domain ideas by theme. Atlas reviews cluster density.

### Dot Voting (Weighted)

Each agent gets a weighted vote allocation based on their domain expertise relevance to the topic. Atlas gets 3 votes on strategic ideas, 1 on operational. Petra gets 3 votes on execution feasibility, 1 on strategy. This prevents the loudest voice from dominating.

### Impact/Effort Matrix

Plot each idea on a 2x2 grid:

- High Impact / Low Effort = **Do First** (quick wins)
- High Impact / High Effort = **Plan Carefully** (major projects)
- Low Impact / Low Effort = **Fill Gaps** (do if time permits)
- Low Impact / High Effort = **Eliminate** (not worth it)

Binky estimates revenue impact. Petra estimates effort. Tina estimates cost. The matrix emerges from their combined assessments.

---

## Lateral Thinking: De Bono's Six Thinking Hats

When a brainstorming session stalls or produces groupthink, rotate through six distinct cognitive modes. Each "hat" forces a different perspective.

| Hat | Focus | Atlas UX Agent Lead |
|-----|-------|-------------------|
| White | Facts and data only. No interpretation. What do we know? What don't we know? | Daily-Intel, Archy |
| Red | Emotions, intuitions, gut feelings. No justification required. | Cheryl (customer empathy), Sunday (audience intuition) |
| Black | Caution, risks, downsides. Devil's advocate. What could go wrong? | Jenny (legal risk), Tina (financial risk) |
| Yellow | Optimism, benefits, best-case scenarios. What's the upside? | Binky (growth upside), Mercer (opportunity) |
| Green | Creativity, alternatives, new ideas. No judgment allowed. | Sunday (creative), All agents contribute |
| Blue | Process control. What hat should we wear next? Are we on track? | Atlas (CEO, facilitates), Petra (PM, tracks) |

Protocol: Atlas calls the hat color. All agents respond through that lens only. No mixing. A full rotation takes six rounds. The Blue Hat (Atlas) decides when to switch and when to conclude.

---

## Multi-Agent Brainwriting: Adapted 6-3-5 Method

The classic 6-3-5 method: 6 people write 3 ideas in 5 minutes, then pass their sheet to the next person who builds on those ideas. Adapted for Atlas UX agents:

### Protocol

1. Atlas defines the problem statement in a single sentence
2. Six agents are selected based on domain relevance
3. Round 1: Each agent generates 3 ideas independently (no visibility into others' ideas)
4. Round 2: Ideas are shuffled. Each agent receives another agent's 3 ideas and must add 3 variations, extensions, or combinations
5. Round 3: Repeat shuffle. Now each agent sees 6 ideas (3 original + 3 from Round 2) and adds 3 more
6. Result: 54 ideas (6 agents x 3 ideas x 3 rounds) with progressive cross-pollination
7. Sunday synthesizes. Atlas evaluates using Impact/Effort matrix

The key advantage: introverted or lower-status agents contribute equally. There is no anchor bias from the first speaker.

---

## Random Stimulus Technique

When conventional thinking produces conventional ideas, inject randomness.

### Method

1. Select a random word, image, or concept unrelated to the problem
2. Force connections between the random stimulus and the problem
3. The forced connections create novel pathways that directed thinking cannot reach

### Agent Application

Archy pulls a random Wikipedia article. Each agent must connect that article's subject to the current business challenge. The connections will range from absurd to brilliant. Even the absurd ones reframe the problem.

Example: Problem is "reduce customer churn." Random stimulus is "coral reef." Connections: coral reefs are ecosystems where every organism depends on others (build interdependencies between features so switching cost is high). Coral bleaching happens when water temperature changes by just 1-2 degrees (small UX friction causes disproportionate churn). Coral reefs attract diverse species (build community features that attract diverse user types who reinforce each other).

---

## Reverse Brainstorming

Instead of "How do we solve X?" ask "How do we make X as bad as possible?"

### Protocol

1. State the problem: "How do we increase customer retention?"
2. Reverse it: "How do we guarantee every customer leaves within 30 days?"
3. Brainstorm answers to the reversed question (this is easy and fun — humans and agents are naturally good at identifying failure modes)
4. Invert each answer back into a solution

Example answers to "how to guarantee churn":
- Make onboarding confusing → Solution: simplify onboarding to 3 steps
- Never respond to support tickets → Solution: guarantee response within 2 hours
- Change the UI every week → Solution: maintain UI consistency, announce changes in advance
- Charge hidden fees → Solution: transparent pricing, no surprises

This technique bypasses the mental block of "I can't think of good ideas" because generating bad ideas is cognitively effortless.

---

## Constraint-Based Creativity

Paradoxically, more constraints produce more creative solutions. Unlimited freedom produces mediocrity.

### Useful Constraints to Impose

- **Time**: "Solve this in 60 seconds of compute time, not 60 minutes"
- **Budget**: "Achieve this with zero additional spend"
- **Resources**: "Do this with only the agents and tools we have today"
- **Scale**: "Design this for 10x our current volume"
- **Inverse scale**: "Design this for a single user"
- **Technology**: "Solve this without AI — how would a human do it?"
- **Geography**: "How would this work in a market where our assumptions don't hold?"

Atlas should periodically impose artificial constraints on brainstorming sessions to prevent lazy solutions that simply throw more resources at problems.

---

## The Atlas UX Brainstorming Protocol

### Standard Operating Procedure

1. **Trigger Detection**: Any agent detects a brainstorming trigger (see conditions above)
2. **Problem Framing**: The detecting agent writes a 1-sentence problem statement using "How might we..." format
3. **Domain Selection**: Atlas selects 4-6 agents based on domain relevance
4. **Divergent Phase** (rounds 1-3): Use SCAMPER, brainwriting, or random stimulus. Goal: quantity over quality. No criticism allowed.
5. **Perspective Phase** (round 4): Run Six Thinking Hats rotation. Goal: examine ideas from all angles.
6. **Convergent Phase** (round 5): Apply affinity mapping, then Impact/Effort matrix. Goal: narrow to top 3-5 ideas.
7. **Documentation**: Sunday writes up the session — problem, participants, all ideas generated, selection rationale, chosen ideas, next actions.
8. **Pipeline Entry**: Chosen ideas enter the Innovation Pipeline (see `adv-innovation-pipeline.md`) at the Screening stage.
9. **Audit**: Session is logged to audit trail with full reasoning chain.

### Quality Metrics for Brainstorming Sessions

- **Idea Volume**: Minimum 30 ideas per session (before convergence)
- **Diversity Score**: Ideas should span at least 4 different categories/approaches
- **Novelty Rate**: At least 20% of ideas should be genuinely new (not variations of existing approaches)
- **Actionability**: At least 50% of ideas post-convergence should be executable within current constraints
- **Cross-Domain Rate**: At least 30% of ideas should combine insights from 2+ agent domains

---

## Anti-Patterns to Avoid

1. **Premature Convergence**: Judging ideas during the divergent phase kills creativity. Separate generation from evaluation.
2. **HIPPO Effect**: Highest Paid Person's Opinion dominates. Atlas must actively solicit from quieter agents before stating a preference.
3. **Anchoring on the First Idea**: The first idea spoken sets an invisible anchor. Use brainwriting to avoid this.
4. **Comfort Zone Recycling**: Generating slight variations of what you already do is not brainstorming. Force novelty through random stimulus or constraints.
5. **Solution Jumping**: Jumping to solutions before fully understanding the problem. Spend 40% of session time on problem framing.
6. **Consensus Seeking**: The goal is not agreement. The goal is the best idea. Healthy disagreement is a feature, not a bug.

---

## Integration with Atlas UX Systems

- Brainstorming sessions create `decision_memo` records when they produce actionable recommendations
- All session outputs are stored in the knowledge base for future reference and pattern mining
- Agents reference prior brainstorming sessions before starting new ones to avoid redundant ideation
- Metrics from brainstorming outcomes feed back into the selection of which techniques to use next
- The engine loop can trigger brainstorming cycles automatically when KPI deviation thresholds are crossed
